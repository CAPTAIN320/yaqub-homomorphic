{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "# Importing the libraries\n",
    "# from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch.nn.parallel\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import tenseal as ts\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.manual_seed(73)\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = th.load(\"data-2/train_X.pt\")\n",
    "train_y = th.load(\"data-2/train_y.pt\")\n",
    "test_X = th.load(\"data-2/test_X.pt\")\n",
    "test_y = th.load(\"data-2/test_y.pt\")\n",
    "#clinical_data = torch.stack(clinical_data,dim=1).squeeze(0) #(torch.arange(clinical_data), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=2)\n",
    "# Test dataset\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDiscriminators():\n",
    "    if (rank != 0):\n",
    "        layer_num = 0\n",
    "        for param in model.parameters():\n",
    "            outdata = param.data.numpy().copy()\n",
    "            indata = None\n",
    "\n",
    "            if (rank != size - 1):\n",
    "                comm.send(outdata, dest=rank + 1, tag=1)\n",
    "            if (rank != 1):\n",
    "                indata = comm.recv(source = rank-1, tag=1)\n",
    "\n",
    "            if (rank == size - 1):\n",
    "                comm.send(outdata, dest=1, tag=2)\n",
    "            if (rank == 1):\n",
    "                indata = comm.recv(source = size - 1, tag=2)\n",
    "            # Shuffling the Discriminator\n",
    "            param.data = torch.from_numpy(indata)\n",
    "            layer_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = out * out\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        shuffleDiscriminators()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        print('Train Epoch: {:2d}   Avg Loss: {:.6f}'.format(epoch, th.mean(th.tensor(losses))))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1   Avg Loss: 0.668271\n",
      "Train Epoch:  2   Avg Loss: 0.610264\n",
      "Train Epoch:  3   Avg Loss: 0.541148\n",
      "Train Epoch:  4   Avg Loss: 0.492275\n",
      "Train Epoch:  5   Avg Loss: 0.458228\n",
      "Train Epoch:  6   Avg Loss: 0.433085\n",
      "Train Epoch:  7   Avg Loss: 0.413479\n",
      "Train Epoch:  8   Avg Loss: 0.397509\n",
      "Train Epoch:  9   Avg Loss: 0.384047\n",
      "Train Epoch: 10   Avg Loss: 0.372386\n",
      "Train Epoch: 11   Avg Loss: 0.362065\n",
      "Train Epoch: 12   Avg Loss: 0.352769\n",
      "Train Epoch: 13   Avg Loss: 0.344287\n",
      "Train Epoch: 14   Avg Loss: 0.336466\n",
      "Train Epoch: 15   Avg Loss: 0.329201\n",
      "Train Epoch: 16   Avg Loss: 0.322416\n",
      "Train Epoch: 17   Avg Loss: 0.316050\n",
      "Train Epoch: 18   Avg Loss: 0.310059\n",
      "Train Epoch: 19   Avg Loss: 0.304405\n",
      "Train Epoch: 20   Avg Loss: 0.299055\n",
      "Train Epoch: 21   Avg Loss: 0.293985\n",
      "Train Epoch: 22   Avg Loss: 0.289173\n",
      "Train Epoch: 23   Avg Loss: 0.284598\n",
      "Train Epoch: 24   Avg Loss: 0.280242\n",
      "Train Epoch: 25   Avg Loss: 0.276089\n",
      "Train Epoch: 26   Avg Loss: 0.272124\n",
      "Train Epoch: 27   Avg Loss: 0.268333\n",
      "Train Epoch: 28   Avg Loss: 0.264704\n",
      "Train Epoch: 29   Avg Loss: 0.261226\n",
      "Train Epoch: 30   Avg Loss: 0.257889\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "model = train(model, device, train_loader, optimizer, criterion, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.92\n"
     ]
    }
   ],
   "source": [
    "def compute_labels(out):\n",
    "    out = th.sigmoid(out)\n",
    "    return (out >= 0.5).int()\n",
    "\n",
    "\n",
    "# compute accuracy using hamming loss\n",
    "def accuracy(output, target):\n",
    "    # convert to labels\n",
    "    out = compute_labels(output)\n",
    "    # flatten and compute hamming loss\n",
    "    flat_out = out.flatten()\n",
    "    flat_target = target.flatten()\n",
    "    incorrect = th.logical_xor(flat_out, flat_target).sum().item()\n",
    "    hamming_loss = incorrect / len(flat_out)\n",
    "    return 1 - hamming_loss\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy(model(test_X), test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is for encrypted model using PyTorch-like model, but which uses TenSEAL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEModel:\n",
    "    def __init__(self, fc1, fc2):\n",
    "        self.fc1_weight = fc1.weight.t().tolist()\n",
    "        self.fc1_bias = fc1.bias.tolist()\n",
    "        self.fc2_weight = fc2.weight.t().tolist()\n",
    "        self.fc2_bias = fc2.bias.tolist()\n",
    "        \n",
    "    def forward(self, encrypted_vec):\n",
    "        # first fc layer + square activation function\n",
    "        encrypted_vec = encrypted_vec.mm(self.fc1_weight) + self.fc1_bias\n",
    "        encrypted_vec *= encrypted_vec\n",
    "        # second fc layer\n",
    "        encrypted_vec = encrypted_vec.mm(self.fc2_weight) + self.fc2_bias\n",
    "        return encrypted_vec\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleHEDiscriminators():\n",
    "    if (rank != 0):\n",
    "        layer_num = 0\n",
    "        for param in he_model.parameters():\n",
    "            outdata = param.data.numpy().copy()\n",
    "            indata = None\n",
    "\n",
    "            if (rank != size - 1):\n",
    "                comm.send(outdata, dest=rank + 1, tag=1)\n",
    "            if (rank != 1):\n",
    "                indata = comm.recv(source = rank-1, tag=1)\n",
    "\n",
    "            if (rank == size - 1):\n",
    "                comm.send(outdata, dest=1, tag=2)\n",
    "            if (rank == 1):\n",
    "                indata = comm.recv(source = size - 1, tag=2)\n",
    "                # Shuffling the Discriminator\n",
    "            param.data = torch.from_numpy(indata)\n",
    "            layer_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bits_scale = 25\n",
    "coeff_mod_bit_sizes = [30, bits_scale, bits_scale, bits_scale, 30]\n",
    "polynomial_modulus_degree = 8192\n",
    "\n",
    "# Create context\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, polynomial_modulus_degree, coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
    "# Set global scale\n",
    "context.global_scale = 2 ** bits_scale\n",
    "# Generate galois keys required for matmul in ckks_vector\n",
    "context.generate_galois_keys()\n",
    "\n",
    "he_model = HEModel(model.fc1, model.fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Encrypted evaluation of entire dataset.\n",
    "Steps:\n",
    "1. encrypt the vector\n",
    "2. do encrypted evaluation\n",
    "3. decrypt the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_train(he_model, device, train_loader, criterion, optimizer, epochs):\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        shuffleHEDiscriminators()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            #clinical_data = torch.stack(clinical_data,dim=1).squeeze(0) #(torch.arange(clinical_data), dim=1)\n",
    "            vec_data = data.flatten()\n",
    "            vec_data = vec_data.flatten()\n",
    "            vec_target = data.flatten()\n",
    "            \n",
    "            encrypted_vec_data = ts.ckks_vector(context, vec_data)\n",
    "            encrypted_vec_target = ts.ckks_vector(context, vec_target)\n",
    "            \n",
    "            encrypted_out_data = he_model(encrypted_vec_data)\n",
    "            encrypted_out_target = he_model(encrypted_vec_target)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = he_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        print('Train Epoch: {:2d}   Avg Loss: {:.6f}'.format(epoch, th.mean(th.tensor(losses))))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
      "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
      "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n",
      "WARNING: The input does not fit in a single ciphertext, and some operations will be disabled.\n",
      "The following operations are disabled in this setup: matmul, matmul_plain, enc_matmul_plain, conv2d_im2col.\n",
      "If you need to use those operations, try increasing the poly_modulus parameter, to fit your input.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can't execute matmul_plain on chunked vectors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18431/1818364171.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhe_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhe_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18431/2247249306.py\u001b[0m in \u001b[0;36mhe_train\u001b[0;34m(he_model, device, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mencrypted_vec_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckks_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mencrypted_out_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhe_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_vec_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mencrypted_out_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhe_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_vec_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_18431/34270295.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_18431/34270295.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encrypted_vec)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencrypted_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# first fc layer + square activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mencrypted_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencrypted_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mencrypted_vec\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mencrypted_vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# second fc layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/GANIN/lib/python3.7/site-packages/tenseal/tensors/ckksvector.py\u001b[0m in \u001b[0;36mmm\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"CKKSVector\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"CKKSVector\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can't execute matmul_plain on chunked vectors"
     ]
    }
   ],
   "source": [
    "he_train(model, device, train_loader, optimizer, criterion, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many labels in the encrypted evaluation are the same as in the plain evaluation?\n",
    "match = 0\n",
    "he_outs = []\n",
    "for data, _ in test_loader:\n",
    "    # remove batch axis, we only need a flat vector\n",
    "    vec = data.flatten()\n",
    "    # encryption\n",
    "    encrypted_vec = ts.ckks_vector(context, vec)\n",
    "    # encrypted evaluation\n",
    "    encrypted_out = he_model(encrypted_vec)\n",
    "    # decryptionhe_epoch = 1\n",
    "    he_out = th.tensor(encrypted_out.decrypt())\n",
    "    he_outs.append(he_out.tolist())\n",
    "    out = model(data)\n",
    "    # how many labels match\n",
    "    he_labels = compute_labels(he_out)\n",
    "    plain_labels = compute_labels(out)\n",
    "    match += (he_labels == plain_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set (encrypted evaluation): {:.2f}\".format(accuracy(th.tensor(he_outs), test_y)))\n",
    "print(\"Encrypted evaluation matched {:.1f}% of the labels from the plain evaluation\".format(\n",
    "    match / (12 * len(test_loader)) * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 party implementation\n",
    "\n",
    "-send the encrypted vector for remote evaluation\n",
    "\n",
    "-then send back encrypted resul for decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
