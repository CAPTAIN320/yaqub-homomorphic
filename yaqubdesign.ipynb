{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Convolutional GANs\n",
    "\n",
    "# Importing the libraries\n",
    "# from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the network to create the peer2peer connection for swaping of the Discriminator\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 640 # We set the size of the batch.\n",
    "imageSize = 64 # We set the size of the generated images (64x64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Resize(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
    "#nc = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "We use dataLoader to get the images of the training set batch by batch.\n",
    "We ust the shuffle = True because we want to get the dataset in random order so that we can train model more precisely.\n",
    "We use num_worker = 2 which represent the number of thread and the worker servers to define the \n",
    "\"\"\"\n",
    "\n",
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the copy of the generator to shuffle between diffrent severs\n",
    "def copyGenerator():\n",
    "    layer_num = 0\n",
    "    for param in netG.parameters():\n",
    "        #print(rank, \"started\")\n",
    "        if (rank == 0):\n",
    "            data = param.data.numpy().copy()\n",
    "            #print(rank, data.shape)\n",
    "        else:\n",
    "            data = None\n",
    "            #print(rank, data.shape)\n",
    "\n",
    "        #print(rank, \"before bcast\")\n",
    "        #comm.Barrier()\n",
    "        data = comm.bcast(data, root = 0)\n",
    "        #print(rank, \"after bcast\")\n",
    "        if (rank != 0):\n",
    "            param.data = torch.from_numpy(data)\n",
    "            #print(\"Node rank \" + str(rank) + \" has synched generator layer \" + str(layer_num))\n",
    "\n",
    "        layer_num += 1\n",
    "        #comm.Barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Peer2Peer shuffling of the Discriminator\n",
    "def shuffleDiscriminators():\n",
    "    if (rank != 0):\n",
    "        layer_num = 0\n",
    "        for param in netD.parameters():\n",
    "            outdata = param.data.numpy().copy()\n",
    "            indata = None\n",
    "\n",
    "            if (rank != size - 1):\n",
    "                comm.send(outdata, dest=rank + 1, tag=1)\n",
    "            if (rank != 1):\n",
    "                indata = comm.recv(source = rank-1, tag=1)\n",
    "\n",
    "            if (rank == size - 1):\n",
    "                comm.send(outdata, dest=1, tag=2)\n",
    "            if (rank == 1):\n",
    "                indata = comm.recv(source = size - 1, tag=2)\n",
    "            # Shuffling the Discriminator\n",
    "            param.data = torch.from_numpy(indata)\n",
    "            layer_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generator\n",
    "\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the generator\n",
    "netG = G()\n",
    "netG.to(device)\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the discriminator\n",
    "netD = D()\n",
    "netD.to(device)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][775/79] Loss_D: 1.7166 Loss_G: 6.5996\n",
      "[0/25][776/79] Loss_D: 1.1719 Loss_G: 5.8026\n",
      "[0/25][777/79] Loss_D: 1.4136 Loss_G: 6.1073\n",
      "[0/25][778/79] Loss_D: 1.3693 Loss_G: 6.3615\n",
      "[0/25][779/79] Loss_D: 1.4312 Loss_G: 6.9311\n",
      "[0/25][780/79] Loss_D: 1.2808 Loss_G: 8.0111\n",
      "[0/25][781/79] Loss_D: 0.8122 Loss_G: 7.2813\n",
      "[0/25][782/79] Loss_D: 0.9929 Loss_G: 9.9526\n",
      "[0/25][783/79] Loss_D: 0.6393 Loss_G: 8.0719\n",
      "[0/25][784/79] Loss_D: 1.3316 Loss_G: 11.4278\n",
      "[0/25][785/79] Loss_D: 0.6379 Loss_G: 8.5148\n",
      "[0/25][786/79] Loss_D: 1.3275 Loss_G: 12.2510\n",
      "[0/25][787/79] Loss_D: 0.4435 Loss_G: 9.6855\n",
      "[0/25][788/79] Loss_D: 0.8021 Loss_G: 11.1497\n",
      "[0/25][789/79] Loss_D: 0.3948 Loss_G: 9.3519\n",
      "[0/25][790/79] Loss_D: 1.0408 Loss_G: 13.7260\n",
      "[0/25][791/79] Loss_D: 0.4272 Loss_G: 10.6118\n",
      "[0/25][792/79] Loss_D: 0.6541 Loss_G: 10.8621\n",
      "[0/25][793/79] Loss_D: 0.3518 Loss_G: 10.5200\n",
      "[0/25][794/79] Loss_D: 0.4639 Loss_G: 11.9108\n",
      "[0/25][795/79] Loss_D: 0.2848 Loss_G: 9.7066\n",
      "[0/25][796/79] Loss_D: 0.9734 Loss_G: 17.2355\n",
      "[0/25][797/79] Loss_D: 0.3707 Loss_G: 15.6894\n",
      "[0/25][798/79] Loss_D: 0.2624 Loss_G: 7.7308\n",
      "[0/25][799/79] Loss_D: 2.9684 Loss_G: 20.6821\n",
      "[0/25][800/79] Loss_D: 0.2725 Loss_G: 22.8957\n",
      "[0/25][801/79] Loss_D: 0.3078 Loss_G: 18.3712\n",
      "[0/25][802/79] Loss_D: 0.1223 Loss_G: 9.9190\n",
      "[0/25][803/79] Loss_D: 0.6295 Loss_G: 13.6581\n",
      "[0/25][804/79] Loss_D: 0.1071 Loss_G: 12.0728\n",
      "[0/25][805/79] Loss_D: 0.1998 Loss_G: 8.0687\n",
      "[0/25][806/79] Loss_D: 1.3313 Loss_G: 21.5034\n",
      "[0/25][807/79] Loss_D: 0.3921 Loss_G: 23.5658\n",
      "[0/25][808/79] Loss_D: 0.2483 Loss_G: 19.5434\n",
      "[0/25][809/79] Loss_D: 0.1200 Loss_G: 11.9619\n",
      "[0/25][810/79] Loss_D: 0.1622 Loss_G: 7.3082\n",
      "[0/25][811/79] Loss_D: 2.3269 Loss_G: 24.7839\n",
      "[0/25][812/79] Loss_D: 0.3575 Loss_G: 28.6449\n",
      "[0/25][813/79] Loss_D: 0.2681 Loss_G: 27.8182\n",
      "[0/25][814/79] Loss_D: 0.1570 Loss_G: 23.6937\n",
      "[0/25][815/79] Loss_D: 0.0749 Loss_G: 15.8896\n",
      "[0/25][816/79] Loss_D: 0.0410 Loss_G: 6.7055\n",
      "[0/25][817/79] Loss_D: 1.4862 Loss_G: 20.7212\n",
      "[0/25][818/79] Loss_D: 0.2624 Loss_G: 23.8965\n",
      "[0/25][819/79] Loss_D: 0.2326 Loss_G: 21.6998\n",
      "[0/25][820/79] Loss_D: 0.1619 Loss_G: 16.4430\n",
      "[0/25][821/79] Loss_D: 0.0611 Loss_G: 9.8893\n",
      "[0/25][822/79] Loss_D: 0.1311 Loss_G: 4.8651\n",
      "[0/25][823/79] Loss_D: 1.4173 Loss_G: 20.4992\n",
      "[0/25][824/79] Loss_D: 0.3417 Loss_G: 23.7976\n",
      "[0/25][825/79] Loss_D: 0.2819 Loss_G: 21.6181\n",
      "[0/25][826/79] Loss_D: 0.1314 Loss_G: 17.0244\n",
      "[0/25][827/79] Loss_D: 0.0864 Loss_G: 10.5643\n",
      "[0/25][828/79] Loss_D: 0.1919 Loss_G: 6.0074\n",
      "[0/25][829/79] Loss_D: 2.4648 Loss_G: 19.2883\n",
      "[0/25][830/79] Loss_D: 0.4169 Loss_G: 21.1843\n",
      "[0/25][831/79] Loss_D: 0.3341 Loss_G: 16.9257\n",
      "[0/25][832/79] Loss_D: 0.2095 Loss_G: 10.3664\n",
      "[0/25][833/79] Loss_D: 0.1749 Loss_G: 5.3480\n",
      "[0/25][834/79] Loss_D: 1.4729 Loss_G: 17.7227\n",
      "[0/25][835/79] Loss_D: 2.1634 Loss_G: 15.5022\n",
      "[0/25][836/79] Loss_D: 0.3833 Loss_G: 10.0320\n",
      "[0/25][837/79] Loss_D: 0.1037 Loss_G: 4.3334\n",
      "[0/25][838/79] Loss_D: 1.1603 Loss_G: 9.4722\n",
      "[0/25][839/79] Loss_D: 0.1908 Loss_G: 9.4998\n",
      "[0/25][840/79] Loss_D: 0.2372 Loss_G: 6.6828\n",
      "[0/25][841/79] Loss_D: 0.3184 Loss_G: 4.9173\n",
      "[0/25][842/79] Loss_D: 0.7065 Loss_G: 7.8327\n",
      "[0/25][843/79] Loss_D: 0.4117 Loss_G: 6.5211\n",
      "[0/25][844/79] Loss_D: 0.2495 Loss_G: 4.5431\n",
      "[0/25][845/79] Loss_D: 0.6312 Loss_G: 8.1069\n",
      "[0/25][846/79] Loss_D: 0.4049 Loss_G: 6.4754\n",
      "[0/25][847/79] Loss_D: 0.2692 Loss_G: 4.1948\n",
      "[0/25][848/79] Loss_D: 0.7808 Loss_G: 8.4735\n",
      "[0/25][849/79] Loss_D: 0.4829 Loss_G: 6.9012\n",
      "[0/25][850/79] Loss_D: 0.3062 Loss_G: 3.8202\n",
      "[0/25][851/79] Loss_D: 0.8114 Loss_G: 8.9505\n",
      "[0/25][852/79] Loss_D: 0.8493 Loss_G: 6.3164\n",
      "[0/25][853/79] Loss_D: 0.4642 Loss_G: 2.7827\n",
      "[1/25][775/79] Loss_D: 1.3746 Loss_G: 10.7270\n",
      "[1/25][776/79] Loss_D: 1.4609 Loss_G: 8.8241\n",
      "[1/25][777/79] Loss_D: 0.4481 Loss_G: 5.0206\n",
      "[1/25][778/79] Loss_D: 0.2346 Loss_G: 3.1185\n",
      "[1/25][779/79] Loss_D: 0.7439 Loss_G: 6.8273\n",
      "[1/25][780/79] Loss_D: 0.5106 Loss_G: 5.5416\n",
      "[1/25][781/79] Loss_D: 0.2943 Loss_G: 3.9161\n",
      "[1/25][782/79] Loss_D: 0.4410 Loss_G: 5.2251\n",
      "[1/25][783/79] Loss_D: 0.2844 Loss_G: 4.7423\n",
      "[1/25][784/79] Loss_D: 0.3821 Loss_G: 3.7668\n",
      "[1/25][785/79] Loss_D: 0.4664 Loss_G: 5.5004\n",
      "[1/25][786/79] Loss_D: 0.5657 Loss_G: 3.0984\n",
      "[1/25][787/79] Loss_D: 0.7442 Loss_G: 7.5476\n",
      "[1/25][788/79] Loss_D: 0.9995 Loss_G: 4.0049\n",
      "[1/25][789/79] Loss_D: 0.3806 Loss_G: 3.9879\n",
      "[1/25][790/79] Loss_D: 0.4055 Loss_G: 5.8449\n",
      "[1/25][791/79] Loss_D: 0.4718 Loss_G: 3.6054\n",
      "[1/25][792/79] Loss_D: 0.5547 Loss_G: 6.0149\n",
      "[1/25][793/79] Loss_D: 0.7229 Loss_G: 3.1132\n",
      "[1/25][794/79] Loss_D: 0.7691 Loss_G: 6.8346\n",
      "[1/25][795/79] Loss_D: 0.9096 Loss_G: 2.5154\n",
      "[1/25][796/79] Loss_D: 0.7056 Loss_G: 6.6689\n",
      "[1/25][797/79] Loss_D: 0.5549 Loss_G: 4.3427\n",
      "[1/25][798/79] Loss_D: 0.2141 Loss_G: 3.4122\n",
      "[1/25][799/79] Loss_D: 0.4239 Loss_G: 5.6860\n",
      "[1/25][800/79] Loss_D: 0.4913 Loss_G: 2.9084\n",
      "[1/25][801/79] Loss_D: 0.6622 Loss_G: 5.5962\n",
      "[1/25][802/79] Loss_D: 0.8973 Loss_G: 3.0140\n",
      "[1/25][803/79] Loss_D: 0.7742 Loss_G: 5.1823\n",
      "[1/25][804/79] Loss_D: 0.5191 Loss_G: 3.3641\n",
      "[1/25][805/79] Loss_D: 0.5938 Loss_G: 6.4087\n",
      "[1/25][806/79] Loss_D: 1.1106 Loss_G: 1.6948\n",
      "[1/25][807/79] Loss_D: 1.3961 Loss_G: 9.3792\n",
      "[1/25][808/79] Loss_D: 2.7606 Loss_G: 4.2527\n",
      "[1/25][809/79] Loss_D: 0.3982 Loss_G: 2.6487\n",
      "[1/25][810/79] Loss_D: 0.8521 Loss_G: 5.8035\n",
      "[1/25][811/79] Loss_D: 1.5231 Loss_G: 1.5187\n",
      "[1/25][812/79] Loss_D: 1.2700 Loss_G: 5.7024\n",
      "[1/25][813/79] Loss_D: 0.7264 Loss_G: 4.2657\n",
      "[1/25][814/79] Loss_D: 0.3470 Loss_G: 2.6985\n",
      "[1/25][815/79] Loss_D: 0.5156 Loss_G: 4.4605\n",
      "[1/25][816/79] Loss_D: 0.4270 Loss_G: 3.3206\n",
      "[1/25][817/79] Loss_D: 0.3898 Loss_G: 3.3005\n",
      "[1/25][818/79] Loss_D: 0.4332 Loss_G: 3.7516\n",
      "[1/25][819/79] Loss_D: 0.5378 Loss_G: 2.9462\n",
      "[1/25][820/79] Loss_D: 0.5763 Loss_G: 4.1490\n",
      "[1/25][821/79] Loss_D: 0.6260 Loss_G: 2.2148\n",
      "[1/25][822/79] Loss_D: 0.8613 Loss_G: 6.4957\n",
      "[1/25][823/79] Loss_D: 1.4585 Loss_G: 2.4823\n",
      "[1/25][824/79] Loss_D: 0.7151 Loss_G: 4.7120\n",
      "[1/25][825/79] Loss_D: 0.5562 Loss_G: 3.0145\n",
      "[1/25][826/79] Loss_D: 0.5168 Loss_G: 4.2117\n",
      "[1/25][827/79] Loss_D: 0.4114 Loss_G: 3.6535\n",
      "[1/25][828/79] Loss_D: 0.5244 Loss_G: 3.6713\n",
      "[1/25][829/79] Loss_D: 0.6065 Loss_G: 3.5076\n",
      "[1/25][830/79] Loss_D: 0.6515 Loss_G: 4.4523\n",
      "[1/25][831/79] Loss_D: 0.6607 Loss_G: 2.3818\n",
      "[1/25][832/79] Loss_D: 1.0122 Loss_G: 7.6483\n",
      "[1/25][833/79] Loss_D: 2.2226 Loss_G: 2.3878\n",
      "[1/25][834/79] Loss_D: 0.8178 Loss_G: 5.7629\n",
      "[1/25][835/79] Loss_D: 1.0436 Loss_G: 2.8956\n",
      "[1/25][836/79] Loss_D: 0.6779 Loss_G: 5.3444\n",
      "[1/25][837/79] Loss_D: 0.6181 Loss_G: 3.1109\n",
      "[1/25][838/79] Loss_D: 1.0036 Loss_G: 7.0037\n",
      "[1/25][839/79] Loss_D: 1.2754 Loss_G: 3.4856\n",
      "[1/25][840/79] Loss_D: 0.6160 Loss_G: 4.0346\n",
      "[1/25][841/79] Loss_D: 0.5276 Loss_G: 5.5820\n",
      "[1/25][842/79] Loss_D: 0.3731 Loss_G: 3.8561\n",
      "[1/25][843/79] Loss_D: 0.4322 Loss_G: 4.6844\n",
      "[1/25][844/79] Loss_D: 0.3571 Loss_G: 4.1172\n",
      "[1/25][845/79] Loss_D: 0.4477 Loss_G: 4.1180\n",
      "[1/25][846/79] Loss_D: 0.4290 Loss_G: 4.0434\n",
      "[1/25][847/79] Loss_D: 0.3978 Loss_G: 4.3273\n",
      "[1/25][848/79] Loss_D: 0.3802 Loss_G: 3.6136\n",
      "[1/25][849/79] Loss_D: 0.4197 Loss_G: 5.2508\n",
      "[1/25][850/79] Loss_D: 0.3938 Loss_G: 3.4588\n",
      "[1/25][851/79] Loss_D: 0.4584 Loss_G: 5.8167\n",
      "[1/25][852/79] Loss_D: 0.4148 Loss_G: 3.5376\n",
      "[1/25][853/79] Loss_D: 0.4364 Loss_G: 5.8942\n",
      "[2/25][775/79] Loss_D: 0.3866 Loss_G: 3.9928\n",
      "[2/25][776/79] Loss_D: 0.4676 Loss_G: 6.3400\n",
      "[2/25][777/79] Loss_D: 0.3271 Loss_G: 4.8326\n",
      "[2/25][778/79] Loss_D: 0.3394 Loss_G: 5.0472\n",
      "[2/25][779/79] Loss_D: 0.2638 Loss_G: 5.6947\n",
      "[2/25][780/79] Loss_D: 0.2686 Loss_G: 4.7259\n",
      "[2/25][781/79] Loss_D: 0.3367 Loss_G: 6.2685\n",
      "[2/25][782/79] Loss_D: 0.3083 Loss_G: 4.4878\n",
      "[2/25][783/79] Loss_D: 0.4133 Loss_G: 8.0253\n",
      "[2/25][784/79] Loss_D: 0.3584 Loss_G: 5.7852\n",
      "[2/25][785/79] Loss_D: 0.2432 Loss_G: 5.6471\n",
      "[2/25][786/79] Loss_D: 0.3189 Loss_G: 7.7026\n",
      "[2/25][787/79] Loss_D: 0.4031 Loss_G: 4.4994\n",
      "[2/25][788/79] Loss_D: 0.6562 Loss_G: 10.6400\n",
      "[2/25][789/79] Loss_D: 0.6514 Loss_G: 8.0360\n",
      "[2/25][790/79] Loss_D: 0.0825 Loss_G: 4.4149\n",
      "[2/25][791/79] Loss_D: 0.7253 Loss_G: 10.4073\n",
      "[2/25][792/79] Loss_D: 0.6232 Loss_G: 7.3733\n",
      "[2/25][793/79] Loss_D: 0.1440 Loss_G: 4.0596\n",
      "[2/25][794/79] Loss_D: 1.0284 Loss_G: 11.8045\n",
      "[2/25][795/79] Loss_D: 1.7497 Loss_G: 8.4739\n",
      "[2/25][796/79] Loss_D: 0.0596 Loss_G: 4.9385\n",
      "[2/25][797/79] Loss_D: 0.3268 Loss_G: 5.4415\n",
      "[2/25][798/79] Loss_D: 0.1816 Loss_G: 6.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/25][799/79] Loss_D: 0.2087 Loss_G: 5.2519\n",
      "[2/25][800/79] Loss_D: 0.2427 Loss_G: 5.4700\n",
      "[2/25][801/79] Loss_D: 0.2411 Loss_G: 6.0013\n",
      "[2/25][802/79] Loss_D: 0.1936 Loss_G: 5.2331\n",
      "[2/25][803/79] Loss_D: 0.2676 Loss_G: 6.0872\n",
      "[2/25][804/79] Loss_D: 0.2265 Loss_G: 5.0264\n",
      "[2/25][805/79] Loss_D: 0.3541 Loss_G: 7.9256\n",
      "[2/25][806/79] Loss_D: 0.4246 Loss_G: 3.9792\n",
      "[2/25][807/79] Loss_D: 0.8997 Loss_G: 12.7910\n",
      "[2/25][808/79] Loss_D: 1.5884 Loss_G: 8.1114\n",
      "[2/25][809/79] Loss_D: 0.0551 Loss_G: 3.8815\n",
      "[2/25][810/79] Loss_D: 0.9689 Loss_G: 10.7440\n",
      "[2/25][811/79] Loss_D: 0.6678 Loss_G: 8.9908\n",
      "[2/25][812/79] Loss_D: 0.0803 Loss_G: 5.8226\n",
      "[2/25][813/79] Loss_D: 0.3944 Loss_G: 7.2173\n",
      "[2/25][814/79] Loss_D: 0.2515 Loss_G: 5.4538\n",
      "[2/25][815/79] Loss_D: 0.2715 Loss_G: 5.7375\n",
      "[2/25][816/79] Loss_D: 0.2514 Loss_G: 5.2868\n",
      "[2/25][817/79] Loss_D: 0.3033 Loss_G: 5.8403\n",
      "[2/25][818/79] Loss_D: 0.2410 Loss_G: 4.4019\n",
      "[2/25][819/79] Loss_D: 0.3012 Loss_G: 6.3560\n",
      "[2/25][820/79] Loss_D: 0.2817 Loss_G: 4.0071\n",
      "[2/25][821/79] Loss_D: 0.2360 Loss_G: 5.6408\n",
      "[2/25][822/79] Loss_D: 0.1169 Loss_G: 5.5820\n",
      "[2/25][823/79] Loss_D: 0.1095 Loss_G: 4.8189\n",
      "[2/25][824/79] Loss_D: 0.1701 Loss_G: 5.0711\n",
      "[2/25][825/79] Loss_D: 0.2142 Loss_G: 5.2908\n",
      "[2/25][826/79] Loss_D: 0.1829 Loss_G: 5.3489\n",
      "[2/25][827/79] Loss_D: 0.2853 Loss_G: 3.5800\n",
      "[2/25][828/79] Loss_D: 0.5155 Loss_G: 10.1483\n",
      "[2/25][829/79] Loss_D: 1.1160 Loss_G: 6.2956\n",
      "[2/25][830/79] Loss_D: 0.0783 Loss_G: 3.7969\n",
      "[2/25][831/79] Loss_D: 0.4384 Loss_G: 6.9295\n",
      "[2/25][832/79] Loss_D: 0.1120 Loss_G: 6.7266\n",
      "[2/25][833/79] Loss_D: 0.1326 Loss_G: 4.8267\n",
      "[2/25][834/79] Loss_D: 0.2778 Loss_G: 5.4104\n",
      "[2/25][835/79] Loss_D: 0.2432 Loss_G: 5.3973\n",
      "[2/25][836/79] Loss_D: 0.2222 Loss_G: 5.2278\n",
      "[2/25][837/79] Loss_D: 0.2535 Loss_G: 5.3736\n",
      "[2/25][838/79] Loss_D: 0.2783 Loss_G: 5.1245\n",
      "[2/25][839/79] Loss_D: 0.2802 Loss_G: 6.5109\n",
      "[2/25][840/79] Loss_D: 0.3752 Loss_G: 3.3041\n",
      "[2/25][841/79] Loss_D: 1.0724 Loss_G: 14.7335\n",
      "[2/25][842/79] Loss_D: 3.6238 Loss_G: 8.9201\n",
      "[2/25][843/79] Loss_D: 0.2564 Loss_G: 2.6492\n",
      "[2/25][844/79] Loss_D: 1.5669 Loss_G: 11.1419\n",
      "[2/25][845/79] Loss_D: 1.9348 Loss_G: 5.9902\n",
      "[2/25][846/79] Loss_D: 0.2240 Loss_G: 3.5734\n",
      "[2/25][847/79] Loss_D: 0.7004 Loss_G: 6.3547\n",
      "[2/25][848/79] Loss_D: 0.5085 Loss_G: 4.6920\n",
      "[2/25][849/79] Loss_D: 0.2575 Loss_G: 3.3383\n",
      "[2/25][850/79] Loss_D: 0.4321 Loss_G: 4.5966\n",
      "[2/25][851/79] Loss_D: 0.2686 Loss_G: 4.2466\n",
      "[2/25][852/79] Loss_D: 0.3930 Loss_G: 3.2625\n",
      "[2/25][853/79] Loss_D: 0.4489 Loss_G: 4.2514\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "for epoch in range(3):\n",
    "\n",
    "    if (epoch % 2 == 0):\n",
    "        shuffleDiscriminators()\n",
    "\n",
    "    for i, data in enumerate(dataloader, 775):\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real).to(device)\n",
    "        target = Variable(torch.ones(input.size()[0])).to(device)\n",
    "        output = netD(input).to(device)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1)).to(device)\n",
    "        fake = netG(noise).to(device)\n",
    "        target = Variable(torch.zeros(input.size()[0])).to(device)\n",
    "        output = netD(fake.detach()).to(device)\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0])).to(device)\n",
    "        output = netD(fake).to(device)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 25, i, len(dataloader), errD.item(), errG.item()))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
