{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9b503496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from typing import Optional\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955a5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_list_from_int(number: int) -> List[int]:\n",
    "    if number < 0 or type(number) is not int:\n",
    "        raise ValueError(\"Only Positive integers are allowed\")\n",
    "\n",
    "    return [int(x) for x in list(bin(number))[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f843304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_even_data(max_int: int, batch_size: int=16)-> Tuple[List[int], List[List[int]]]:\n",
    "    # Get the number of binary places needed to represent the maximum number\n",
    "    max_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Sample batch_size number of integers in range 0-max_int\n",
    "    sampled_integers = np.random.randint(0, int(max_int / 2), batch_size)\n",
    "\n",
    "    # create a list of labels all ones because all numbers are even\n",
    "    labels = [1] * batch_size\n",
    "\n",
    "    # Generate a list of binary numbers for training.\n",
    "    data = [create_binary_list_from_int(int(x * 2)) for x in sampled_integers]\n",
    "    data = [([0] * (max_length - len(x))) + x for x in data]\n",
    "\n",
    "    return labels, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "025dffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.dense_layer = nn.Linear(int(input_length), int(input_length))\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fc84614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_length: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense = nn.Linear(int(input_length), 1);\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.dense(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6eff6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "context = ts.context(\n",
    "            ts.SCHEME_TYPE.CKKS,\n",
    "            poly_modulus_degree=8192,\n",
    "            coeff_mod_bit_sizes=[60, 40, 40, 60]\n",
    "          )\n",
    "context.generate_galois_keys()\n",
    "context.global_scale = 2**40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "8cd2f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 4096\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 20\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()\n",
    "\n",
    "\n",
    "# parameters\n",
    "poly_mod_degree = 8192\n",
    "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
    "# create TenSEALContext\n",
    "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "ctx_training.global_scale = 2 ** 21\n",
    "ctx_training.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "5a18e376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(max_int: int = 128, batch_size: int = 16, training_steps: int = 500):\n",
    "    input_length = int(math.log(max_int, 2))\n",
    "\n",
    "    # Models\n",
    "    generator = Generator(input_length)\n",
    "    discriminator = Discriminator(input_length)\n",
    "\n",
    "    # Optimizers\n",
    "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "    # loss\n",
    "    loss = nn.BCELoss()\n",
    "\n",
    "    for i in range(training_steps):\n",
    "        # zero the gradients on each iteration\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Create noisy input for generator\n",
    "        # Need float type instead of int\n",
    "        noise = torch.randint(0, 2, size=(batch_size, input_length)).float()\n",
    "        generated_data = generator(noise)\n",
    "\n",
    "        # Generate examples of even real data\n",
    "        true_labels, true_data = generate_even_data(max_int, batch_size=batch_size)\n",
    "        true_labels = torch.tensor(true_labels).float()\n",
    "        true_labels = true_labels.unsqueeze(1)\n",
    "        true_data = torch.tensor(true_data).float()\n",
    "        \n",
    "        # Train the generator\n",
    "        # We invert the labels here and don't train the discriminator because we want the generator\n",
    "        # to make things the discriminator classifies as true.\n",
    "        generator_discriminator_out = discriminator(generated_data)\n",
    "        generator_loss = loss(generator_discriminator_out, true_labels)\n",
    "        generator_loss_vector = [generator_loss.item()]\n",
    "        print(generator_loss_vector)\n",
    "        enc_generator_loss = ts.ckks_vector(ctx_training, generator_loss_vector)\n",
    "        print(enc_generator_loss)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Train the discriminator on the true/generated data\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        true_discriminator_out = discriminator(true_data)\n",
    "        true_discriminator_loss = loss(true_discriminator_out, true_labels)\n",
    "\n",
    "        # add .detach() here think about this\n",
    "        generator_discriminator_out = discriminator(generated_data.detach())\n",
    "        #generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size))\n",
    "        generator_discriminator_loss = loss(generator_discriminator_out, torch.zeros(batch_size).unsqueeze(1))\n",
    "        discriminator_loss = (true_discriminator_loss + generator_discriminator_loss) / 2\n",
    "        discriminator_loss.backward()\n",
    "        discriminator_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "15b8a14c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5542437434196472]\n",
      "[0.5585272312164307]\n",
      "[0.5629175305366516]\n",
      "[0.5583036541938782]\n",
      "[0.5547553896903992]\n",
      "[0.5575031638145447]\n",
      "[0.561896800994873]\n",
      "[0.5681354999542236]\n",
      "[0.5544306039810181]\n",
      "[0.5640419125556946]\n",
      "[0.5715162754058838]\n",
      "[0.5620367527008057]\n",
      "[0.5720278024673462]\n",
      "[0.5731748938560486]\n",
      "[0.5752259492874146]\n",
      "[0.5704964399337769]\n",
      "[0.5724347829818726]\n",
      "[0.5706439018249512]\n",
      "[0.5712560415267944]\n",
      "[0.5730897188186646]\n",
      "[0.575793981552124]\n",
      "[0.5837514400482178]\n",
      "[0.58234703540802]\n",
      "[0.5748985409736633]\n",
      "[0.580812931060791]\n",
      "[0.5775256752967834]\n",
      "[0.581829309463501]\n",
      "[0.5832788944244385]\n",
      "[0.5819604992866516]\n",
      "[0.5805007219314575]\n",
      "[0.5813208222389221]\n",
      "[0.5842555165290833]\n",
      "[0.5858339071273804]\n",
      "[0.5876688957214355]\n",
      "[0.5919299125671387]\n",
      "[0.5834435224533081]\n",
      "[0.5870649814605713]\n",
      "[0.5863118767738342]\n",
      "[0.5915297269821167]\n",
      "[0.5977877974510193]\n",
      "[0.5833285450935364]\n",
      "[0.593869149684906]\n",
      "[0.5963711142539978]\n",
      "[0.5984539985656738]\n",
      "[0.6011722683906555]\n",
      "[0.593683123588562]\n",
      "[0.6016730070114136]\n",
      "[0.5964047312736511]\n",
      "[0.6004985570907593]\n",
      "[0.5977942943572998]\n",
      "[0.5987273454666138]\n",
      "[0.5955169200897217]\n",
      "[0.6019898653030396]\n",
      "[0.5987972617149353]\n",
      "[0.601803183555603]\n",
      "[0.5988455414772034]\n",
      "[0.6036157608032227]\n",
      "[0.5977215766906738]\n",
      "[0.5983160138130188]\n",
      "[0.604430079460144]\n",
      "[0.6031103134155273]\n",
      "[0.6003299951553345]\n",
      "[0.605270266532898]\n",
      "[0.6098407506942749]\n",
      "[0.6056089401245117]\n",
      "[0.6042604446411133]\n",
      "[0.6068826913833618]\n",
      "[0.6079578995704651]\n",
      "[0.609573245048523]\n",
      "[0.6034385561943054]\n",
      "[0.6106775999069214]\n",
      "[0.6052768230438232]\n",
      "[0.6068674325942993]\n",
      "[0.6136432886123657]\n",
      "[0.6114130020141602]\n",
      "[0.6102246046066284]\n",
      "[0.6103485822677612]\n",
      "[0.6152635812759399]\n",
      "[0.6189562678337097]\n",
      "[0.6128478050231934]\n",
      "[0.6150426864624023]\n",
      "[0.6120815873146057]\n",
      "[0.6123818159103394]\n",
      "[0.6151371002197266]\n",
      "[0.6244168877601624]\n",
      "[0.6132611632347107]\n",
      "[0.613477349281311]\n",
      "[0.6197187900543213]\n",
      "[0.6189953088760376]\n",
      "[0.6227556467056274]\n",
      "[0.6110411882400513]\n",
      "[0.6153626441955566]\n",
      "[0.619749903678894]\n",
      "[0.6181621551513672]\n",
      "[0.6225797533988953]\n",
      "[0.6135003566741943]\n",
      "[0.6189478635787964]\n",
      "[0.6239829063415527]\n",
      "[0.6201561689376831]\n",
      "[0.614149272441864]\n",
      "[0.6109086871147156]\n",
      "[0.6216903924942017]\n",
      "[0.6222667694091797]\n",
      "[0.6231799125671387]\n",
      "[0.6217978596687317]\n",
      "[0.6170585751533508]\n",
      "[0.6223454475402832]\n",
      "[0.6241172552108765]\n",
      "[0.6210009455680847]\n",
      "[0.621921718120575]\n",
      "[0.6190783977508545]\n",
      "[0.6213200092315674]\n",
      "[0.6205704212188721]\n",
      "[0.6237998008728027]\n",
      "[0.6265152096748352]\n",
      "[0.6163702607154846]\n",
      "[0.6225012540817261]\n",
      "[0.6212494373321533]\n",
      "[0.6259922981262207]\n",
      "[0.6138697862625122]\n",
      "[0.6215981245040894]\n",
      "[0.6262849569320679]\n",
      "[0.611213207244873]\n",
      "[0.6227041482925415]\n",
      "[0.6129165291786194]\n",
      "[0.6213341951370239]\n",
      "[0.6132298707962036]\n",
      "[0.6217862367630005]\n",
      "[0.6267764568328857]\n",
      "[0.6169607639312744]\n",
      "[0.620195209980011]\n",
      "[0.6247550845146179]\n",
      "[0.6196881532669067]\n",
      "[0.622162938117981]\n",
      "[0.6258240938186646]\n",
      "[0.6224886178970337]\n",
      "[0.6265636682510376]\n",
      "[0.6203294992446899]\n",
      "[0.6279174089431763]\n",
      "[0.6188082695007324]\n",
      "[0.6200686693191528]\n",
      "[0.622221827507019]\n",
      "[0.6242423057556152]\n",
      "[0.6140373349189758]\n",
      "[0.6193207502365112]\n",
      "[0.6217085123062134]\n",
      "[0.6212698817253113]\n",
      "[0.6130076050758362]\n",
      "[0.620452880859375]\n",
      "[0.6235325336456299]\n",
      "[0.6209481358528137]\n",
      "[0.6159163117408752]\n",
      "[0.6188581585884094]\n",
      "[0.62038254737854]\n",
      "[0.6095176935195923]\n",
      "[0.6161313056945801]\n",
      "[0.6190035343170166]\n",
      "[0.6212704181671143]\n",
      "[0.6201399564743042]\n",
      "[0.6221499443054199]\n",
      "[0.6196386814117432]\n",
      "[0.6195912957191467]\n",
      "[0.6178316473960876]\n",
      "[0.6214807629585266]\n",
      "[0.621967077255249]\n",
      "[0.6243982315063477]\n",
      "[0.6221560835838318]\n",
      "[0.6205915212631226]\n",
      "[0.6296470761299133]\n",
      "[0.6245644092559814]\n",
      "[0.6213997602462769]\n",
      "[0.6170201301574707]\n",
      "[0.6241730451583862]\n",
      "[0.6263068914413452]\n",
      "[0.6228168606758118]\n",
      "[0.6221268177032471]\n",
      "[0.6301661133766174]\n",
      "[0.6245623826980591]\n",
      "[0.628339409828186]\n",
      "[0.6311272978782654]\n",
      "[0.6269909143447876]\n",
      "[0.6232262849807739]\n",
      "[0.6282597184181213]\n",
      "[0.6264845132827759]\n",
      "[0.6322945952415466]\n",
      "[0.6280907392501831]\n",
      "[0.630989134311676]\n",
      "[0.6310387253761292]\n",
      "[0.6236096024513245]\n",
      "[0.6385220289230347]\n",
      "[0.6300448179244995]\n",
      "[0.6230023503303528]\n",
      "[0.6308250427246094]\n",
      "[0.6341684460639954]\n",
      "[0.6285919547080994]\n",
      "[0.6338766813278198]\n",
      "[0.6331200003623962]\n",
      "[0.6269664764404297]\n",
      "[0.6341366767883301]\n",
      "[0.6293784976005554]\n",
      "[0.6243688464164734]\n",
      "[0.6278814673423767]\n",
      "[0.6309764981269836]\n",
      "[0.640923261642456]\n",
      "[0.6360506415367126]\n",
      "[0.6307365894317627]\n",
      "[0.6359366178512573]\n",
      "[0.6315276622772217]\n",
      "[0.6375976800918579]\n",
      "[0.6372631788253784]\n",
      "[0.6271345615386963]\n",
      "[0.637420117855072]\n",
      "[0.6334722638130188]\n",
      "[0.6305550932884216]\n",
      "[0.6337901949882507]\n",
      "[0.6333522200584412]\n",
      "[0.6290079951286316]\n",
      "[0.6393837332725525]\n",
      "[0.6344527006149292]\n",
      "[0.6304479241371155]\n",
      "[0.6381161212921143]\n",
      "[0.6356462240219116]\n",
      "[0.6367919445037842]\n",
      "[0.6327674984931946]\n",
      "[0.6285051107406616]\n",
      "[0.6336534023284912]\n",
      "[0.6347827315330505]\n",
      "[0.6415418386459351]\n",
      "[0.6368211507797241]\n",
      "[0.6439727544784546]\n",
      "[0.6325892210006714]\n",
      "[0.6342452764511108]\n",
      "[0.6403819918632507]\n",
      "[0.6360735297203064]\n",
      "[0.6355551481246948]\n",
      "[0.6380772590637207]\n",
      "[0.6418935060501099]\n",
      "[0.6458783149719238]\n",
      "[0.637633740901947]\n",
      "[0.6418249011039734]\n",
      "[0.6335220336914062]\n",
      "[0.6359641551971436]\n",
      "[0.6447877883911133]\n",
      "[0.6447051763534546]\n",
      "[0.6368504166603088]\n",
      "[0.6438699960708618]\n",
      "[0.6390930414199829]\n",
      "[0.6413108706474304]\n",
      "[0.6448801755905151]\n",
      "[0.6449435353279114]\n",
      "[0.6486496925354004]\n",
      "[0.643676221370697]\n",
      "[0.6407701373100281]\n",
      "[0.6448962688446045]\n",
      "[0.6436925530433655]\n",
      "[0.6444907784461975]\n",
      "[0.6497983336448669]\n",
      "[0.6476361751556396]\n",
      "[0.6428796052932739]\n",
      "[0.643399178981781]\n",
      "[0.647004246711731]\n",
      "[0.6492612361907959]\n",
      "[0.6481145620346069]\n",
      "[0.6486992835998535]\n",
      "[0.6481915712356567]\n",
      "[0.6443193554878235]\n",
      "[0.6511627435684204]\n",
      "[0.6471121311187744]\n",
      "[0.6483073234558105]\n",
      "[0.6452237367630005]\n",
      "[0.6531777381896973]\n",
      "[0.6606958508491516]\n",
      "[0.6487516164779663]\n",
      "[0.6499828696250916]\n",
      "[0.6510024666786194]\n",
      "[0.6570456624031067]\n",
      "[0.6520956158638]\n",
      "[0.647135317325592]\n",
      "[0.6506223678588867]\n",
      "[0.6521744728088379]\n",
      "[0.6520266532897949]\n",
      "[0.6548735499382019]\n",
      "[0.665703296661377]\n",
      "[0.6544638276100159]\n",
      "[0.6549907922744751]\n",
      "[0.6521772742271423]\n",
      "[0.6539175510406494]\n",
      "[0.6515575647354126]\n",
      "[0.6601311564445496]\n",
      "[0.6516373157501221]\n",
      "[0.6525800228118896]\n",
      "[0.6575068235397339]\n",
      "[0.6547254920005798]\n",
      "[0.6549999117851257]\n",
      "[0.6631399393081665]\n",
      "[0.65651935338974]\n",
      "[0.6576484441757202]\n",
      "[0.6588854193687439]\n",
      "[0.6555076837539673]\n",
      "[0.6628882884979248]\n",
      "[0.6569364666938782]\n",
      "[0.654863715171814]\n",
      "[0.6634384989738464]\n",
      "[0.659153163433075]\n",
      "[0.6531627774238586]\n",
      "[0.6575844287872314]\n",
      "[0.6614735126495361]\n",
      "[0.6617684960365295]\n",
      "[0.6613065600395203]\n",
      "[0.6595812439918518]\n",
      "[0.6562480330467224]\n",
      "[0.6613799929618835]\n",
      "[0.6609991788864136]\n",
      "[0.6610885858535767]\n",
      "[0.6601071357727051]\n",
      "[0.658033549785614]\n",
      "[0.6621965169906616]\n",
      "[0.66157466173172]\n",
      "[0.6602110266685486]\n",
      "[0.6651207208633423]\n",
      "[0.6612213850021362]\n",
      "[0.6606603264808655]\n",
      "[0.6650192737579346]\n",
      "[0.6677641868591309]\n",
      "[0.6636985540390015]\n",
      "[0.6624444127082825]\n",
      "[0.6612048149108887]\n",
      "[0.6665958166122437]\n",
      "[0.666857123374939]\n",
      "[0.6665632724761963]\n",
      "[0.6721091270446777]\n",
      "[0.664422333240509]\n",
      "[0.6687164902687073]\n",
      "[0.6738889813423157]\n",
      "[0.6582357883453369]\n",
      "[0.6689249873161316]\n",
      "[0.6638699173927307]\n",
      "[0.6667433977127075]\n",
      "[0.6690378189086914]\n",
      "[0.6649820804595947]\n",
      "[0.661518394947052]\n",
      "[0.6724129915237427]\n",
      "[0.6698564291000366]\n",
      "[0.6700745820999146]\n",
      "[0.6688458919525146]\n",
      "[0.6727136373519897]\n",
      "[0.6700409054756165]\n",
      "[0.6759135723114014]\n",
      "[0.6695186495780945]\n",
      "[0.6711605191230774]\n",
      "[0.6700747013092041]\n",
      "[0.6746320724487305]\n",
      "[0.6728084683418274]\n",
      "[0.6687777638435364]\n",
      "[0.6708741188049316]\n",
      "[0.6738092303276062]\n",
      "[0.6718823909759521]\n",
      "[0.6738703846931458]\n",
      "[0.6662622690200806]\n",
      "[0.6701704263687134]\n",
      "[0.6796201467514038]\n",
      "[0.673438549041748]\n",
      "[0.6769666075706482]\n",
      "[0.680715024471283]\n",
      "[0.6728137135505676]\n",
      "[0.6736108064651489]\n",
      "[0.6764082908630371]\n",
      "[0.6727364659309387]\n",
      "[0.6818292737007141]\n",
      "[0.6760268807411194]\n",
      "[0.678593099117279]\n",
      "[0.6789083480834961]\n",
      "[0.6729034781455994]\n",
      "[0.6792961359024048]\n",
      "[0.6778208017349243]\n",
      "[0.6833028793334961]\n",
      "[0.6843054890632629]\n",
      "[0.6915534734725952]\n",
      "[0.6793757677078247]\n",
      "[0.6817144751548767]\n",
      "[0.6863071322441101]\n",
      "[0.6904352903366089]\n",
      "[0.687889575958252]\n",
      "[0.6911749839782715]\n",
      "[0.6818605065345764]\n",
      "[0.6901029944419861]\n",
      "[0.6925898790359497]\n",
      "[0.6854139566421509]\n",
      "[0.6975240707397461]\n",
      "[0.6907875537872314]\n",
      "[0.6875919103622437]\n",
      "[0.6937128305435181]\n",
      "[0.6864951252937317]\n",
      "[0.6908678412437439]\n",
      "[0.6916470527648926]\n",
      "[0.6946293115615845]\n",
      "[0.6894366145133972]\n",
      "[0.6909831762313843]\n",
      "[0.6943110227584839]\n",
      "[0.6932991147041321]\n",
      "[0.6984424591064453]\n",
      "[0.6987083554267883]\n",
      "[0.6955701112747192]\n",
      "[0.6949319243431091]\n",
      "[0.6962884664535522]\n",
      "[0.6949542760848999]\n",
      "[0.698775589466095]\n",
      "[0.693860650062561]\n",
      "[0.695220947265625]\n",
      "[0.7017189264297485]\n",
      "[0.7046451568603516]\n",
      "[0.7005276679992676]\n",
      "[0.6989408731460571]\n",
      "[0.7029777765274048]\n",
      "[0.6990682482719421]\n",
      "[0.7031014561653137]\n",
      "[0.7058744430541992]\n",
      "[0.7038935422897339]\n",
      "[0.7042621970176697]\n",
      "[0.6982913017272949]\n",
      "[0.7081857919692993]\n",
      "[0.707625687122345]\n",
      "[0.7043746709823608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7062872648239136]\n",
      "[0.7067877650260925]\n",
      "[0.7042995691299438]\n",
      "[0.7159066796302795]\n",
      "[0.7074021100997925]\n",
      "[0.7121376991271973]\n",
      "[0.7011553049087524]\n",
      "[0.7057291269302368]\n",
      "[0.7110608816146851]\n",
      "[0.7087165117263794]\n",
      "[0.7048594951629639]\n",
      "[0.7086701393127441]\n",
      "[0.7157617807388306]\n",
      "[0.7113911509513855]\n",
      "[0.7015479803085327]\n",
      "[0.7106163501739502]\n",
      "[0.7073631286621094]\n",
      "[0.7150470614433289]\n",
      "[0.7114810943603516]\n",
      "[0.7150009870529175]\n",
      "[0.7139045000076294]\n",
      "[0.7183828353881836]\n",
      "[0.7127599120140076]\n",
      "[0.7078262567520142]\n",
      "[0.7149680256843567]\n",
      "[0.7110747694969177]\n",
      "[0.7137323617935181]\n",
      "[0.7155201435089111]\n",
      "[0.7106055617332458]\n",
      "[0.7155376672744751]\n",
      "[0.7156375050544739]\n",
      "[0.7090803980827332]\n",
      "[0.714834451675415]\n",
      "[0.7197393774986267]\n",
      "[0.7169337868690491]\n",
      "[0.7125425934791565]\n",
      "[0.7220710515975952]\n",
      "[0.7208635807037354]\n",
      "[0.7198382616043091]\n",
      "[0.717902421951294]\n",
      "[0.7161601781845093]\n",
      "[0.7276496291160583]\n",
      "[0.7137881517410278]\n",
      "[0.7197854518890381]\n",
      "[0.7173445224761963]\n",
      "[0.7186065912246704]\n",
      "[0.7172107100486755]\n",
      "[0.7258067727088928]\n",
      "[0.723587155342102]\n",
      "[0.7283663153648376]\n",
      "[0.7253528833389282]\n",
      "[0.7213615775108337]\n",
      "[0.726985514163971]\n",
      "[0.7277685403823853]\n",
      "[0.7296200394630432]\n",
      "[0.7263906002044678]\n",
      "[0.7262567281723022]\n",
      "[0.7256354093551636]\n",
      "[0.7236175537109375]\n",
      "[0.7297441363334656]\n",
      "[0.7303191423416138]\n",
      "[0.7276866436004639]\n",
      "[0.7272350192070007]\n",
      "[0.7380732297897339]\n",
      "[0.7305199503898621]\n",
      "[0.7298975586891174]\n",
      "[0.7328458428382874]\n",
      "[0.7332481741905212]\n",
      "[0.732782244682312]\n",
      "[0.7396568059921265]\n",
      "[0.7309722304344177]\n",
      "[0.7372259497642517]\n",
      "[0.7312510013580322]\n",
      "[0.7357919216156006]\n",
      "[0.7330387830734253]\n",
      "[0.7375876903533936]\n",
      "[0.7384578585624695]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ccf43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "fa396b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[[1, 1, 0, 0, 0, 1, 0], [1, 0, 1, 0, 1, 1, 0], [1, 1, 0, 0, 1, 1, 0], [1, 1, 0, 0, 1, 0, 0], [0, 1, 1, 1, 1, 0, 0], [0, 1, 0, 1, 1, 1, 0], [1, 0, 1, 0, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0], [0, 0, 1, 0, 0, 1, 0], [1, 1, 0, 1, 1, 1, 0], [0, 0, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "y_train, x_train = generate_even_data(max_int=128, batch_size=16)\n",
    "print(y_train)\n",
    "y_train\n",
    "print(x_train)\n",
    "x_test = [115, 90, 94, 122, 122, 122, 95, 115, 83, 123, 114, 123, 94, 89, 91, 86]\n",
    "y_test = [0,1,1,1,1,1,0,0,0,0,1,0,1,0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "c1c0cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(LR, self).__init__()\n",
    "        self.lr = torch.nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.sigmoid(self.lr(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "34b628a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features = x_train.shape[1]\n",
    "n_features = 7\n",
    "model = LR(n_features)\n",
    "# use gradient descent with a learning_rate=1\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1)\n",
    "# use Binary Cross Entropy Loss\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "6771da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncryptedLR:\n",
    "    \n",
    "    def __init__(self, torch_lr):\n",
    "        self.weight = torch_lr.lr.weight.data.tolist()[0]\n",
    "        self.bias = torch_lr.lr.bias.data.tolist()\n",
    "        # we accumulate gradients and counts the number of iterations\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "        \n",
    "    def forward(self, enc_x):\n",
    "        enc_out = enc_x.dot(self.weight) + self.bias\n",
    "        enc_out = EncryptedLR.sigmoid(enc_out)\n",
    "        return enc_out\n",
    "    \n",
    "    def backward(self, enc_x, enc_out, enc_y):\n",
    "        out_minus_y = (enc_out - enc_y)\n",
    "        self._delta_w += enc_x * out_minus_y\n",
    "        self._delta_b += out_minus_y\n",
    "        self._count += 1\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        if self._count == 0:\n",
    "            raise RuntimeError(\"You should at least run one forward iteration\")\n",
    "        # update weights\n",
    "        # We use a small regularization term to keep the output\n",
    "        # of the linear layer in the range of the sigmoid approximation\n",
    "        self.weight -= self._delta_w * (1 / self._count) + self.weight * 0.05\n",
    "        self.bias -= self._delta_b * (1 / self._count)\n",
    "        # reset gradient accumulators and iterations count\n",
    "        self._delta_w = 0\n",
    "        self._delta_b = 0\n",
    "        self._count = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(enc_x):\n",
    "        # We use the polynomial approximation of degree 3\n",
    "        # sigmoid(x) = 0.5 + 0.197 * x - 0.004 * x^3\n",
    "        # from https://eprint.iacr.org/2018/462.pdf\n",
    "        # which fits the function pretty well in the range [-5,5]\n",
    "        return enc_x.polyval([0.5, 0.197, 0, -0.004])\n",
    "    \n",
    "    def plain_accuracy(self, x_test, y_test):\n",
    "        # evaluate accuracy of the model on\n",
    "        # the plain (x_test, y_test) dataset\n",
    "        w = torch.tensor(self.weight)\n",
    "        b = torch.tensor(self.bias)\n",
    "        out = torch.sigmoid(x_test.matmul(w) + b).reshape(-1, 1)\n",
    "        correct = torch.abs(y_test - out) < 0.5\n",
    "        return correct.float().mean()    \n",
    "    \n",
    "    def encrypt(self, context):\n",
    "        self.weight = ts.ckks_vector(context, self.weight)\n",
    "        self.bias = ts.ckks_vector(context, self.bias)\n",
    "        \n",
    "    def decrypt(self):\n",
    "        self.weight = self.weight.decrypt()\n",
    "        self.bias = self.bias.decrypt()\n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2bf85a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "poly_mod_degree = 4096\n",
    "coeff_mod_bit_sizes = [40, 20, 40]\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** 20\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()\n",
    "\n",
    "\n",
    "# parameters\n",
    "poly_mod_degree = 8192\n",
    "coeff_mod_bit_sizes = [40, 21, 21, 21, 21, 21, 21, 40]\n",
    "# create TenSEALContext\n",
    "ctx_training = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, -1, coeff_mod_bit_sizes)\n",
    "ctx_training.global_scale = 2 ** 21\n",
    "ctx_training.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539eeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "17064d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encryption of the training_set took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "t_start = time()\n",
    "#enc_x_train = [ts.ckks_vector(ctx_training, x.tolist()) for x in x_train]\n",
    "#enc_y_train = [ts.ckks_vector(ctx_training, y.tolist()) for y in y_train]\n",
    "enc_x_train_array = []\n",
    "enc_y_train_array = []\n",
    "for x,y in zip(x_train,y_train):\n",
    "    enc_x_train = ts.ckks_vector(ctx_training, x)\n",
    "    enc_x_train_array.append(enc_x_train)\n",
    "    #print(\"Encryption of \", x,\" is \", enc_x_train)\n",
    "    \n",
    "    # convert single digit to vector\n",
    "    y = [int(d) for d in str(y)]\n",
    "    enc_y_train = ts.ckks_vector(ctx_training, y)\n",
    "    enc_y_train_array.append(enc_y_train)\n",
    "    #print(\"Encryption of \", y,\" is \", enc_y_train)\n",
    "\n",
    "    \n",
    "    \n",
    "#enc_x_train = [ts.ckks_vector(ctx_training, x_train) for x in x_train]\n",
    "#enc_y_train = [ts.ckks_vector(ctx_training, y_train) for y in y_train]\n",
    "                                 \n",
    "t_end = time()\n",
    "print(f\"Encryption of the training_set took {int(t_end - t_start)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "fd0f2a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average time per epoch: 1 seconds\n"
     ]
    }
   ],
   "source": [
    "eelr = EncryptedLR(LR(n_features))\n",
    "#accuracy = eelr.plain_accuracy(x_test, y_test)\n",
    "#print(f\"Accuracy at epoch #0 is {accuracy}\")\n",
    "EPOCHS = 5\n",
    "\n",
    "times = []\n",
    "for epoch in range(EPOCHS):\n",
    "    eelr.encrypt(ctx_training)\n",
    "    \n",
    "    # if you want to keep an eye on the distribution to make sure\n",
    "    # the function approxiamation is still working fine\n",
    "    # WARNING: this operation is time consuming\n",
    "    # encrypted_out_distribution(eelr, enc_x_train)\n",
    "    \n",
    "    t_start = time()\n",
    "    for enc_x, enc_y in zip(enc_x_train_array, enc_y_train_array):\n",
    "        enc_out = eelr.forward(enc_x)\n",
    "        eelr.backward(enc_x, enc_out, enc_y)\n",
    "     \n",
    "    eelr.update_parameters()\n",
    "    t_end = time()\n",
    "    times.append(t_end - t_start)\n",
    "    \n",
    "    eelr.decrypt()\n",
    "    #accuracy = eelr.plain_accuracy(x_test, y_test)\n",
    "    #print(f\"Accuracy at epoch #{epoch + 1} is {accuracy}\")\n",
    "\n",
    "\n",
    "print(f\"\\nAverage time per epoch: {int(sum(times) / len(times))} seconds\")\n",
    "#print(f\"Final accuracy is {accuracy}\")\n",
    "\n",
    "#diff_accuracy = plain_accuracy - accuracy\n",
    "#print(f\"Difference between plain and encrypted accuracies: {diff_accuracy}\")\n",
    "#if diff_accuracy < 0:\n",
    "#    print(\"Oh! We got a better accuracy when training on encrypted data! The noise was on our side...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1fe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
