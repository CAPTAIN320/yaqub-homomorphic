{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8be08dd610>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tenseal as ts\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "th.manual_seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = th.load(\"data-2/train_X.pt\")\n",
    "train_y = th.load(\"data-2/train_y.pt\")\n",
    "test_X = th.load(\"data-2/test_X.pt\")\n",
    "test_y = th.load(\"data-2/test_y.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "# Test dataset\n",
    "test_dataset = TensorDataset(test_X, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 128)\n",
    "        self.fc2 = nn.Linear(128, 12)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = out * out\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, epochs):\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        print('Train Epoch: {:2d}   Avg Loss: {:.6f}'.format(epoch, th.mean(th.tensor(losses))))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  1   Avg Loss: 0.681389\n",
      "Train Epoch:  2   Avg Loss: 0.617955\n",
      "Train Epoch:  3   Avg Loss: 0.546425\n",
      "Train Epoch:  4   Avg Loss: 0.497110\n",
      "Train Epoch:  5   Avg Loss: 0.462913\n",
      "Train Epoch:  6   Avg Loss: 0.437628\n",
      "Train Epoch:  7   Avg Loss: 0.417854\n",
      "Train Epoch:  8   Avg Loss: 0.401683\n",
      "Train Epoch:  9   Avg Loss: 0.387986\n",
      "Train Epoch: 10   Avg Loss: 0.376063\n",
      "Train Epoch: 11   Avg Loss: 0.365469\n",
      "Train Epoch: 12   Avg Loss: 0.355911\n",
      "Train Epoch: 13   Avg Loss: 0.347191\n",
      "Train Epoch: 14   Avg Loss: 0.339169\n",
      "Train Epoch: 15   Avg Loss: 0.331745\n",
      "Train Epoch: 16   Avg Loss: 0.324841\n",
      "Train Epoch: 17   Avg Loss: 0.318395\n",
      "Train Epoch: 18   Avg Loss: 0.312357\n",
      "Train Epoch: 19   Avg Loss: 0.306684\n",
      "Train Epoch: 20   Avg Loss: 0.301341\n",
      "Train Epoch: 21   Avg Loss: 0.296295\n",
      "Train Epoch: 22   Avg Loss: 0.291520\n",
      "Train Epoch: 23   Avg Loss: 0.286990\n",
      "Train Epoch: 24   Avg Loss: 0.282685\n",
      "Train Epoch: 25   Avg Loss: 0.278587\n",
      "Train Epoch: 26   Avg Loss: 0.274677\n",
      "Train Epoch: 27   Avg Loss: 0.270942\n",
      "Train Epoch: 28   Avg Loss: 0.267369\n",
      "Train Epoch: 29   Avg Loss: 0.263944\n",
      "Train Epoch: 30   Avg Loss: 0.260658\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = th.device(\"cpu\")\n",
    "\n",
    "model = train(model, device, train_loader, optimizer, criterion, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.93\n"
     ]
    }
   ],
   "source": [
    "def compute_labels(out):\n",
    "    out = th.sigmoid(out)\n",
    "    return (out >= 0.5).int()\n",
    "\n",
    "\n",
    "# compute accuracy using hamming loss\n",
    "def accuracy(output, target):\n",
    "    # convert to labels\n",
    "    out = compute_labels(output)\n",
    "    # flatten and compute hamming loss\n",
    "    flat_out = out.flatten()\n",
    "    flat_target = target.flatten()\n",
    "    incorrect = th.logical_xor(flat_out, flat_target).sum().item()\n",
    "    hamming_loss = incorrect / len(flat_out)\n",
    "    return 1 - hamming_loss\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.2f}\".format(accuracy(model(test_X), test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is for encrypted model using PyTorch-like model, but which uses TenSEAL operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HEModel:\n",
    "    def __init__(self, fc1, fc2):\n",
    "        self.fc1_weight = fc1.weight.t().tolist()\n",
    "        self.fc1_bias = fc1.bias.tolist()\n",
    "        self.fc2_weight = fc2.weight.t().tolist()\n",
    "        self.fc2_bias = fc2.bias.tolist()\n",
    "        \n",
    "    def forward(self, encrypted_vec):\n",
    "        # first fc layer + square activation function\n",
    "        encrypted_vec = encrypted_vec.mm(self.fc1_weight) + self.fc1_bias\n",
    "        encrypted_vec *= encrypted_vec\n",
    "        # second fc layer\n",
    "        encrypted_vec = encrypted_vec.mm(self.fc2_weight) + self.fc2_bias\n",
    "        return encrypted_vec\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bits_scale = 25\n",
    "coeff_mod_bit_sizes = [30, bits_scale, bits_scale, bits_scale, 30]\n",
    "polynomial_modulus_degree = 8192\n",
    "\n",
    "# Create context\n",
    "context = ts.context(ts.SCHEME_TYPE.CKKS, polynomial_modulus_degree, coeff_mod_bit_sizes=coeff_mod_bit_sizes)\n",
    "# Set global scale\n",
    "context.global_scale = 2 ** bits_scale\n",
    "# Generate galois keys required for matmul in ckks_vector\n",
    "context.generate_galois_keys()\n",
    "\n",
    "he_model = HEModel(model.fc1, model.fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is Encrypted evaluation of entire dataset.\n",
    "Steps:\n",
    "1. encrypt the vector\n",
    "2. do encrypted evaluation\n",
    "3. decrypt the result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many labels in the encrypted evaluation are the same as in the plain evaluation?\n",
    "match = 0\n",
    "he_outs = []\n",
    "for data, _ in test_loader:\n",
    "    # remove batch axis, we only need a flat vector\n",
    "    vec = data.flatten()\n",
    "    # encryption\n",
    "    encrypted_vec = ts.ckks_vector(context, vec)\n",
    "    # encrypted evaluation\n",
    "    encrypted_out = he_model(encrypted_vec)\n",
    "    # decryption\n",
    "    he_out = th.tensor(encrypted_out.decrypt())\n",
    "    he_outs.append(he_out.tolist())\n",
    "    out = model(data)\n",
    "    # how many labels match\n",
    "    he_labels = compute_labels(he_out)\n",
    "    plain_labels = compute_labels(out)\n",
    "    match += (he_labels == plain_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set (encrypted evaluation): {:.2f}\".format(accuracy(th.tensor(he_outs), test_y)))\n",
    "print(\"Encrypted evaluation matched {:.1f}% of the labels from the plain evaluation\".format(\n",
    "    match / (12 * len(test_loader)) * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2 party implementation\n",
    "\n",
    "-send the encrypted vector for remote evaluation\n",
    "\n",
    "-then send back encrypted resul for decryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
